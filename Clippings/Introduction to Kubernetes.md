---
title: "Course Transcript"
source: "https://cdn2.percipio.com/secure/c/1762338284.efda73f8179b14534c53cf928959eb97f58a1352/eot/transcripts/8382372a-59aa-448b-acfa-c79199694f45/it_dowwctdj_03_enus.html"
author:
published:
created: 2025-11-04
description:
tags:
  - "clippings"
---
## Working with Containers: Introduction to Kubernetes

No matter what the infrastructure is that you are using to manage your applications, you are likely to find challenges trying to keep up when scaling out as demand for resources increases. The answer to this problem is automation and orchestration, and one of the most common tools for providing automation and orchestration for containers is called Kubernetes. Explore this open-source container orchestration system. You will gain insight into the various terms associated with this software and investigate the common workflow used to produce containers and deploy software to scale. Then, you will discover how to secure a Kubernetes Cluster. Finally, you will examine Kubernetes use cases and identify the differences between Kubernetes and Docker. After course completion, you will be able to work with Kubernetes containers, outline the Kubernetes workflow, and determine its situational usefulness.

## Table of Contents

## 1\. Video: Course Overview (it\_dowwctdj\_03\_enus\_01)

![](https://cdn2.percipio.com/public/b/5c0b4991-eff1-473e-b96a-1420481bc6b8/image001.jpg)

In this video, discover the key concepts covered in this course.

- discover the key concepts covered in this course

\[Video description begins\] *Topic title: Course Overview. Your host for this session is Elias Zoghaib.* \[Video description ends\]

No matter what the infrastructure is that you are using to manage your applications, you are likely to find challenges trying to keep up when scaling out as demand for resources increases. The answer to this problem is automation and orchestration, and one of the most common tools for providing that for containers is called Kubernetes.

In this course, I'll discuss this open-source container orchestration system. I'll give insight into the various terms associated with this software and then start progressing into the common workflow used to produce containers and deployed software to scale. I'll also cover how to use Kubernetes to deploy software and how to secure a Kubernetes cluster.

At the end of this course, I'll examine Kubernetes use cases and identify the differences between Kubernetes and Docker.

## 2\. Video: Kubernetes Foundation (it\_dowwctdj\_03\_enus\_02)

![](https://cdn2.percipio.com/public/b/5e60238b-b907-42a8-9158-06e6bad7fef5/image001.jpg)

In this video, you will learn how to describe the service Kubernetes provides.

- describe the service that Kubernetes provides

\[Video description begins\] *Topic title: Kubernetes Foundation. Your host for this session is Elias Zoghaib.* \[Video description ends\]

In this video, I'll give a brief introduction to Kubernetes. Kubernetes is an open-source, powerful system created by Google. It manages containerized applications in a clustered environment. It aims to provide better ways of managing related distributed components and services across varied infrastructure. Kubernetes uses containerized applications as its bread and butter. Since, we've used Docker before to run a single instance of an application, say something like an API, we can use these containerized applications with Docker and give them to Kubernetes to automatically scale the services up or down.

The way it does this is it takes your individual container and runs them across a cluster of machines. This cluster has a few different pieces to it, and we'll talk about that in just a moment, but understanding that Kubernetes is just managing the app lifecycle. And an app lifecycle just means that it can be starting or stopping or restarting the container application. Kubernetes handles all of these problems that you'll see with a containerized application. Few different things that a Kubernetes users can do.

Kubernetes user can define how your application should run and there's ways that you should be able to interact with other applications or the outside world. One of the ways is it can scale the services up or down. You can scale the services up or down or perform graceful rolling updates such as a blue-green deployment and switch traffic between different versions of your applications to test features or roll back problematic deployments. Kubernetes provides interfaces and composable platform primitives that allow you to define and manage your applications with high degrees of flexibility, power, and reliability.

As a Kubernetes user, you'll be working with what's called a Kubernetes cluster. What this means is that it's just a set of nodes that run containerized applications. You can think of a node as just a computer with a certain amount of RAM, certain number of CPUs, maybe some SSD or HDD for storage, but it can run an operating system such as Linux, Red Hat, CentOS, or even Ubuntu. There are more lightweight and flexible than virtual machines, but containerizing these application packages an app with its dependencies and some necessary services that these nodes will be able to run.

In this way, Kubernetes clusters allow for applications to be more easily developed, moved, and managed. Kubernetes clusters allow containers to run across multiple machines and environment such as virtual, physical, cloud-based, or even on-premises. Kubernetes containers are not restricted to a specific operating system, unlike virtual machines. Instead, they are able to share operating systems and run anywhere. Let's talk about the individual pieces of what a cluster is. A cluster contains one master node. The master node controls the state of the cluster, for example, which applications are running and their corresponding container images.

The master node is the origin for all task assignments. So, there's things like scheduling or maintaining a cluster state, or implementing updates are all handled by the master node. Now, the other piece to a cluster is known as several worker nodes. There could be more than one of these. A worker node are components that run applications. Worker nodes perform tasks assigned by the master node. They can either be virtual machines or even physical computers, but they're all operating as part of 1 system controlled by the master node, and we'll talk a bit about how they interface with the master node in just a moment.

But they use these primary components of Kubernetes to handle the communication between the master node and the worker node. One of these components is known as the API server. This exposes a rest interface to all Kubernetes resources. It serves as the front end of the Kubernetes control plane. In fact, the command line tool that controls Kubernetes, such as qctl, is using this API server that will take commands from your command line.

There's also a scheduler. This places containers according to resource requirements and metrics. It makes notes of Pods with no assigned node and selects nodes to run them on. And then there's controller managers. This runs controller processes and reconciles the cluster's actual state with its desired specifications. This controller manager is useful for understanding when I should have multiple versions of my application running on the cluster. Perhaps higher amount of traffic is being expected at this time, the controller manager will spin up more containers for this service. The kubelet ensures that containers are running in a Pod by interacting with the Docker engine. This is the default program for creating and managing containers. kube-proxy is the networking piece of Kubernetes.

This manages network connectivity and maintains network rules across the nodes. It implements the Kubernetes service concept across every node in a given cluster. It's essential for having nodes communicate with each other. And finally, there's the etcd. This is the key-value store that's installed on the master node. It stores all cluster data. It's consistent and highly available Kubernetes backing store. These 6 components are what form the Kubernetes cluster.

The master node runs the API server, the scheduler, and the controller manager, and the worker nodes runs the kubelet and the kube-proxy. Few different things that some terminologies we're going to be seeing with Kubernetes clusters is determine the desired state. Now, a desired state say if I'm running an application on a Kubernetes cluster, I'd like there to be 3 containers running my API for example, that is considered a desired state. At all times, there should be 3 containers running this API server. If for whatever reason one of these containers goes down, some error happens on one of the machines, Kubernetes will spin up the extra container so that it reaches the desired state.

We use a manifest to define this desired state and manifest is simply just a JSON or YAML file that's used to specify the application type and the number of replicas needed to run the system. And then there's the Kubernetes API to define a cluster's desired state. The developer interaction uses the command line interface known as Kubectl, or it leverages the API to directly interact with the cluster to manually set the desired state. The master node will then communicate the desired state to the worker nodes via its own API.

And finally, because Kubernetes is a automatic system, what this means is that it will automatically manage clusters to align with their desired state through this Kubernetes control plane. This means that I don't have to worry about downtime for my individual applications. Since I've already told Kubernetes what I'd like the desired state to be, it'll work with the manifest and the Kubernetes API itself to make sure that the system is running according to my specifications. So, that's just a brief introduction of Kubernetes and that's all I have for this video.

## 3\. Video: Kubernetes Terms (it\_dowwctdj\_03\_enus\_03)

![](https://cdn2.percipio.com/public/b/97f91c8c-5d99-49df-8434-13230277fa32/image001.jpg)

- define key terms associated with Kubernetes

\[Video description begins\] *Topic title: Kubernetes Terms. Your host for this session is Elias Zoghaib.* \[Video description ends\]

In this video, I'll be discussing some key Kubernetes terms that we're going to be seeing in the next few videos. Kubernetes uses what's called the Kubernetes API. This API is the lifeblood of the system. You may have heard of the Kubernetes described as a declarative tool, what this really means is you're saying this is how I want things to run, and that it does what's needed to make that happen in a highly automated way. I don't have to worry about the individual networking pieces Kubernetes will handle everything. Those little details are all handled by the software. The next term that you're going to be seeing is known as cluster.

Now, you can begin to understand this major piece literally. Think of what a cluster is. It's a group or a bunch of nodes that run your containerized applications. You manage the cluster and everything it includes. In other words, you manage your applications with Kubernetes. Couple more key terms, there's the Kubernetes control plane. This sits between a cluster and Kubernetes basically as a necessary intermediary. It makes sure everything behaves properly, like a chaperone and a container dance party. When people use automation as one of the key benefits of Kubernetes and container orchestration, this is a key piece. Now, from directly from the Kubernetes website you can see that they describe the control plane as maintaining a record of all the Kubernetes objects in the system and runs continuous control loops to manage those objects' states.

This just means that something is going to be constantly checking to make sure those objects are exactly the way I've declared it. Another term is known as node. Nodes are compromised of physical or virtual machines on your cluster. These worker machines have everything necessary to run your application containers, including the container runtime, usually Docker and other critical services like a Kubernetes GitHub repository has a good detailed breakdown of a Kubernetes node and what's actually installed there by default. Another term is known as pod. This is essentially the smallest deployable unit of the Kubernetes ecosystem. More accurately, it's the smallest object, period.

Pod specifically represents a group of one or more containers running together on your cluster. Just a bit of a best practice is you want to try and keep similar containers in one pod. As in, don't have your database container and your application container sitting in the same pod. You kind of want to separate them based on like their role, and then there's what's called a master. A master maintains the desired state of your cluster. You will commonly see it referred to as the master node. Every cluster has a master node as well as several worker nodes. The master node includes 3 critical processes for managing the state of your cluster.

There's the kube API server, kube controller manager, and the kube-scheduler. When you make changes, you're almost always making them to the master node, and the master node will take those changes and send them to all the worker nodes. So, you're actually just managing one machine instead of four or five, depending on how many are in your worker group. Volumes, a volume is simply a directory of data. It lives within a pod and can be accessed by any other container running in that pod. A volume is the abstraction that lets Kubernetes deals with the ephemeral nature of containers. What this means is containers can go down anytime and we don't want to, say, have to restart our data all over again every single time our data goes down.

What should happen is the container should be writing all of its data to a particular volume, and in case the container goes down, it has a backup of the data. So, Kubernetes will spin up another container inside of that pod and connect it to that volume. So, you won't even notice that something was restarted. And then there's what's called persistent volume. Now, this is ephemerality of data, persistent volumes deal with the issue of storage that needs to exist outside of the lifetime of any particular container or application or as general volumes deal with compute, this becomes particularly important, when you're discussing stateful applications like databases. Kubectl, simply put, is just a command line interface for managing the operations on your Kubernetes clusters.

It does so by communicating with the Kubernetes API. So, what this actually means is it follows a standard syntax for running commands. There's kubectl space, the command name, space a type whether it's node or master, and then space, the name of the command, and then a flag. You can find an in-depth explanation of this kubectl here, but you'll see that it follows a general formula. And then there's minikube. Minikube is a tool for running Kubernetes on a local machine such as laptop. You'll likely hear more about this as we're going to be building Kubernetes test clusters using our laptop or current machine that's running it, just as a test to see if everything is going to work properly on a much bigger machine.

There's kubelet, an agent that runs on each node in the cluster. It makes sure that containers are running in a pod. The kubelet takes a set of pod specs that are provided through various mechanisms and ensures that the containers described in those pod specs are running and healthy. The kubelet doesn't manage containers, that's actually done by something else. Manifests are specifications of a Kubernetes API object in JSON or YAML format. Manifests specifies the desired state of an object the Kubernetes will maintain when you apply the manifest. Each configuration file can contain multiple manifests. And the last two terms I want to talk about is known as object an entity in the Kubernetes system. This object is the API uses these entities to represent the state of your cluster.

Kubernetes's object is typically usually just a record of intent. Once you create the object, the Kubernetes control plane will work constantly to ensure that the item represents actually exists. And then there's the control plane. The container orchestration layer that exposes the API and interfaces it to define, deploy and manage the lifecycle of containers is called the control plane. This layer is composed by many different components. But it's not restricted to things like the API server, scheduler, or the control manager, so these are just some of the common terms that you're going to be seeing in Kubernetes.

## 4\. Video: The Kubernetes Workflow (it\_dowwctdj\_03\_enus\_04)

![](https://cdn2.percipio.com/public/b/091b1fb7-6d35-43d2-9672-223696f951de/image001.jpg)

In this video, discover how to identify the steps of the overall Kubernetes workflow.

- identify the steps of the overall Kubernetes workflow

\[Video description begins\] *Topic title: TheKubernetes Workflow. Your host for this session is Elias Zoghaib.* \[Video description ends\]

In this video, I'll be discussing the Kubernetes workflow. How exactly do we send commands from say a terminal to our Kubernetes cluster? Well, as you recall, in Kubernetes there's a master node. This master node contains the API server, the controller manager, scheduler, and the key-value store etcd. When we're sending kubectl commands, we're not actually talking to any of the worker nodes. We're talking specifically to the API server that's running on the master node. The API server is what's going to be sending information to the different worker nodes, kubelet, or the controller manager, the scheduler, or the etcd key-value store, depending on the command that's going to be talking to these objects.

Now, the worker nodes contain what's called a kubelet, or in a kube-proxy to manage the networking between the different pods. Now, over here you'll notice that there's a pod. The pod contains multiple containers which are Docker runtime instances, and then this is also a pod with three containers inside of it.  
  
\[Video description begins\] *A slide appears with the heading Kubernetes Workflow. It displays a workflow in various stages. The first and top block or stage is labeled as an API server. This API server is called as a Master block. It leads to 3 blocks or 3 sub-stages below that: controller-manager, scheduler, and etcd. It further leads to 2 Worker blocks. Each block consists of 2 elements: kubelet and kube-proxy. Both elements contain 2 sub-elements, which consist of various containers. These elements are connected by uni-directional arrows.* \[Video description ends\]  
  
So, you can almost see that this is almost like a nested entity. You've got together the worker node, which contains multiple pods, and each pod can contain multiple containers. So with that, that's the idea of what's happening. What's going on now is that the API server is going to receive our kubectl command. You can think of it as let's say for example I want to run a specific command with the kubectl command line interface tool, say it's something kubectl space create space the name of container, I'd like to create.

This command will spawn a container, but the way it does it is it first communicates with the API server running on the master node. When that API server receives the information from the command line, it's going to make these calls specifically to that server, and then afterwards, using the REST API, it's going to validate these REST API calls and the kube-apiserver understands the task and calls the kube-scheduler. The scheduler will select one node from the available ones to execute the job. In this case, it could be one of the worker nodes. This is what's known as the scheduling procedure. Once the kube-scheduler returns the target node, the kube-apiserver will dispatch the task with all of the details describing the task.

The kubelet process in that target node receives the task and talks to the container engine, for example, the Docker Engine. So, what's shown here? This will spawn a container with all the provided parameters. This job and its specification will be recorded in a centralized database known as etcd. Its job is to preserve and provide access to all the data in that cluster. Let's take a look at this step by step. So, we saw that the kubectl client is going to translate the command specifically into a few REST API calls. It's going to send those API calls to the apiserver. The apiserver is going to send it to a scheduler. The scheduler is going to submit these tasks to the apiserver, all the schedulers what it's doing is finding a worker node that's available.

Once that worker node has been targeted by the scheduler, it's going to get the apiserver to submit tasks specifically to that worker node. The worker node will receive that task and it will process in this case or spawn a new container. Afterwards, it's going to record that it did this job in the database known as etcd. So, the Kubernetes workflow also involves job status monitoring. Along every step, we get to see exactly how this job in this case just to spawn a new container on one available worker node is doing. How's it progressing? Is it running? Are we waiting? Could there be other jobs that are there before it? Job status monitoring tool will give us an idea of what's happening.

Now, this is just a simplified workflow, just spawning a new container, but they can get very complex, and having a status monitoring tool is going to help us. Another thing it has is the kube-controller-manager. This is what's actually handling the entire API call between API server and the scheduler. And finally, there's the abstraction layer. Have you actually noticed throughout this process we're not actually working with any Docker containers specifically? What we're doing is we're using an abstraction layer. We're working with higher-level objects by creating these kubectl commands and sending it to an API server. The API server and the scheduler are working together to control whatever is happening on the system itself.

In this case, we're not actually talking to the Docker container running on the worker node, but the API server is doing that kind of translation for us. This is the beauty of Kubernetes. It allows us to make these very minutia detailed calls to a worker node without actually having to write any more specific code. So, that's all I had today about the Kubernetes workflow.

## 5\. Video: Desired State in Kubernetes (it\_dowwctdj\_03\_enus\_05)

![](https://cdn2.percipio.com/public/b/ee1eaeb2-e4af-4dcf-a4cd-c90b83c3fe3c/image001.jpg)

Explore and identify the elements that determine a desired state in Kubernetes.

- describe and identify the elements that determine a desired state in Kubernetes

\[Video description begins\] *Topic title: Desired State in Kubernetes. Your host for this session is Elias Zoghaib.* \[Video description ends\]

In this video, I'll be discussing Kubernetes desired state. The desired state in Kubernetes involves what's called declarative imperative API. What this means is that it's going to be using the API to describe the state of an object, could be a pod, a replica set, or a deployment that will run the containers. We're also going to be using this API to describe the desired state. This is one of the core components of Kubernetes and it's one of the reasons why people like to use Kubernetes so much because its allows us just to say I like my state to be this, do whatever it takes to make sure that my state remains at this level.

Kubernetes via its kube-controller-manager, it's part of the master components are in charge of regulating the state of the system. This kubelet component is what's going to be, it's located on each of the worker nodes and it receives requests and applies the new configuration to run these containers. There's also known as the API objects. The state, the configuration, the API objects, they're all stored in a simple distributed key-value storage called the etcd cluster. These API objects are what's going to be useful for us to maintain what's called the desired state. All of this is being done under the hood through Kubernetes.

Every time you use what's called a manifest to describe a desired state, it'll use all of these tools. The kube-controller-manager on the master node, the kubelet manager on the worker node, the API object stored inside of the etcd cluster to manage itself to make sure that it's staying at this desired state that's been selected. So, Kubernetes strictly ensures that all of the containers running across the cluster are always in this desired state. And the way it does this is that you have to specify those instances, and they're usually specified in the manifest. And you can use, say, something like a simple deployment object, which is something that can be defined with a YAML file, and then create that deployment all across your Kubernetes.

But you've defined that whole deployment inside of a YAML file. Using a simple command I can apply that YAML file to my Kubernetes cluster. Kubernetes determines the required actions. That's all you have to do. You save your desired state, you send it using the command kubectl command to apply that deployment file and from there Kubernetes will determine whatever actions it has to do to make sure that everything is working accordingly. Few more instances of desired state is that if you are familiar with Kubernetes config files, you've already saw that the API version, and the kind of fields that the beginning of each file they describe the object type, its version and group specifically using a different apiVersion will have a different keyword storage.

So, you'll see something at the very top of these files, and it can be a little confusing at first, but it'll be something like apps/v1/beta2 or something like apps/v1, and even without these group, so you may not even have to have a beta2 or beta1. But things are moving really fast in the Kubernetes world and different apiVersions are coming out all the time. So, you need to ask the API server to the CLI for kubectl. What apiVersion you're going to be using? And that has to be specified. So, how do you know which versions to use? Well, depends on your kubectl. So, I can actually in my command line interface, I can type the command kubectl space api-versions and it will speed out all the possible versions I could be using in my manifest file. So, the API server and the CLI will tell me exactly what's supported and I can use any of those apiVersions in my manifest.

Now, a couple of things to bring up one of the strengths of Kubernetes, and that's just the fact that it's you can set your system, give a desired state, and it will just do everything it needs to do to make sure that you're at that desired state. You no longer have to write your own software to make sure that your servers are running accordingly. Kubernetes will handle all of that stuff for you, and of course, it aligns with basic GitOps tools because you're using a declarative functions, Git operations are going to be very simple for us to use. You just have to create these CI/CD pipelines on your Git operations page, say something like GitHub for example, and every time you push to a repository.

What I can do is create an automated build step that will create this deployment file and deploy it specifically on the kubectl for my production server or my staging server or even my dev server. I can just create those flows. Every time I decide to do a PR into a particular branch, that build step will trigger and it'll automatically show up on my Kubernetes cluster where I can view it in a web browser. So, that's all I have to say about desired state and Kubernetes.

## 6\. Video: Kubernetes Resource Provisioning (it\_dowwctdj\_03\_enus\_06)

![](https://cdn2.percipio.com/public/b/e287cafe-b824-40d7-94ea-1cfe9ecc4b36/image001.jpg)

In this video, you will discover the provisioning process for Kubernetes.

- describe the provisioning process of Kubernetes

\[Video description begins\] *Topic title:Kubernetes Resource Provisioning. Your host for this session is Elias Zoghaib.* \[Video description ends\]

This video will be discussing the cluster lifecycle in Kubernetes. The cluster lifecycle in Kubernetes can be divided in several parts. The first part is known as the design step. This step contains all of the configuration settings you're going to have for your worker nodes. How many worker nodes you'll have? How much CPU will be in each node? How much RAM will be in each node? You'll also discuss what pods are going to be created. How many services will be we created from these pods? How many containers will be in each pod and what they'll be doing? All of that is done at the design step.

Once your configuration is determined for your purpose, you can move on to the deployment step. The deployment step just means that we're going to be taking our manifest and deploying it on a Kubernetes cluster that we've created using a cloud provider, for example. After that deployment is done, we're going to be at the operation step. This is where we're going to be deploying our service to particular clients or a user base, and they're going to be making API calls to our Kubernetes cluster. They're going to be going to the domains of every service, and the service is going to be doing a particular action, and returning your result, maybe using an HTTP protocol such as REST.

Once we satisfy the operation metric and we'd like to do a cleanup, maybe we'd like to turn off our services, we can do the deletion step. This involves stopping all services, deleting all pods, removing any volumes that are leftover, and then taking apart the worker nodes turning them off. And then finally deleting the master node. Kubernetes offers what's called a multi-cluster. So, we've talked about a single cluster, how it can provision clusters that run and manage our workloads. Depending on the needs of an organization, however, Kubernetes deployments can be replicated, should have the same workloads accessible across multiple nodes and environments.

This concept is called Kubernetes multi-cluster orchestration. It's simply provisioning your workloads in several Kubernetes clusters. Going beyond just a single cluster, a Kubernetes multi-cluster defines deployment strategies to introduce scalability, availability, and isolation for your workloads and environments. It's a fully embraced when an organization coordinates the planning, delivery, and management of several Kubernetes environments using appropriate tools and processes. And why would we need to use a multi-cluster? Well, in certain advanced deployments it's not enough just to have a single cluster, some cases need these advanced deployment models and for such scenarios, a multi-cluster architecture is suitable and can improve the performance of your workloads.

Simply put, a development team may need a Kubernetes multicluster to handle workloads spanning regions and manage compliance requirements, or maybe soft multitenancy conflicts and enforce security around clusters and tenants. So, how do we spin up these multi-clusters? Well, there's some tools that will help us doing that. Spinning Kubernetes clusters without tools or automations can be a headache and I suggest you don't go that route. You can save yourself some significant challenges arising from the complexities and operational overhead by using tools and managed services that are already available across several cloud platforms. These are some of the tools that I like to use. There's Terraform that spins Kubernetes clusters as an uphill task.

But using Terraform, which is a cross-cloud tool, its cloud agnostic, it can work on AWS, Google Cloud, and Azure. An infrastructure as a code tool such as Terraform simply provisions Kubernetes clusters using code. It allows you to declare infrastructure resources you want to create and apply the declarations at the command line level. You can also invoke Terraform commands that will spin those resources for you without writing a script at all. Using Terraform you can spin these clusters on any cloud provider, although you could use the native Kubernetes kubectl tools for full lifecycle management for your resources, but Terraform offers these own benefits. And one of them is just having a one command, one tool to handle all of your cloud provisioning across several different cloud providers.

Another tool that I like to use is Amazon EKS. This is a elastic Kubernetes service that's offered by Amazon that will handle all of your configuration steps for between your worker nodes and your master node. You don't have to worry about the individual network creations or the subnets. All you have to do is provide the configuration for the worker nodes such as the number of CPUs and the amount of RAM maybe if they have any type of external storage attached to them and you select the region for your cluster and Amazon will handle the Kubernetes cluster creation on its own. Finally, there's kOps. This is an automated tool for provisioning, upgrading, maintaining, and deleting production-grade, highly available Kubernetes clusters. Additionally, kOps can provision necessary cloud infrastructure to work with your clusters. This tool integrates well with platforms such as AWS, GCE, and VMware.

If you're on Amazon, I highly recommend you use the EKS tool. It's an excellent managed service. But if you're trying to create multiple clusters across multiple different cloud providers, I suggest you use Terraform and kOps. And then finally the last couple of tools that I like to use and this is known as a Rancher. This is a great GUI tool to manage your Kubernetes. It's a container management platform that provides Kubernetes as a service with built-in support for a multi-cluster orchestration or interest, supports the spinning of clusters on various environments like on-premises, data centers, cloud, and edge clusters that run anywhere. An in-built tool like Rancher Kubernetes engine can provide developers with great flexibility and it comes with full integration support for tools such as CI/CD.

Additionally, Rancher is a unified multi-cluster platform that addresses operational and security issues around several Kubernetes clusters. Rancher just watches your Kubernetes clusters through centrally configured security policies for centralized authentication and access control, enterprise-grade security, auditing, backups, and alerts. This way, developers can get consistently spin secure clusters anywhere in a matter of minutes using a web interface that's offered by Rancher on the local host. Finally, there's Loft. This is a platform with several tool sets that offers managed self-service solutions for you to spin and scale clusters smoothly. Loft features self-service environment provisioning, secure Kubernetes multitenancy, and enterprise-grade access control. Loft is 100% Kubernetes, so you can control everything via native Kubernetes tools and APIs, It's still in integration with other cloud-native tools.

It's a great tool if you're just looking to spin up a simple Kubernetes cluster. So, the basic provisioning steps of clusters are package application in a Docker container image. That's the first step. You need to have your application ready to go and an image, from that image you're going to be containing many different containers. Once that's image is created you need to host that image on a container repository or image repository. This is something like Docker Hub or AWS has its own or Google has its own or you can just upload your image directly to that Hub. You can use minikube to set up a Kubernetes cluster.

This is a great tool for setting up a cluster locally to make sure that your Docker image is going to run exactly the way you expect on something small that's provisioned by your local computer. And finally, there's deploying image on a Kubernetes cluster. This is running the simple command that deploys your system using your YAML file that you build manifest. And from there, your Kubernetes will pull down the image from the Docker Hub or the image repository that you've uploaded your Docker image to, and it'll start running your services. And that's all there is to it. So, that's all I have about multi-clusters in Kubernetes.

## 7\. Video: Application Deployment in Kubernetes (it\_dowwctdj\_03\_enus\_07)

![](https://cdn2.percipio.com/public/b/d1ff9c09-2f51-4b34-8ce9-e8e8ffedd89e/image001.jpg)

After completing this video, you will be able to describe the application deployment phase when using Kubernetes.

- describe the application deployment phase when using Kubernetes

\[Video description begins\] *Topic title: Application Deployment in Kubernetes. Your host for this session is Elias Zoghaib.* \[Video description ends\]

In this video, we'll be discussing the different steps to deploying an application in Kubernetes. Since now we understand what pods are and we start with an application, we'd like to figure out the individual steps and the building blocks that are required to take an application that's already started with say something a simple API with a get request that says hello world, and we would like to take that application and put this into a Docker container and put that container inside of a pod and put the pod inside of a deployment. So, to start, we're going to have to take our application that's running on our local host and prepare to run it on Docker.

This step just means that we're going to Dockerize the application. Dockerizing the application involves taking our different steps that we had to do, such as installing dependencies and different programming languages and starting with the base image of a Docker container like Ubuntu or Python 3.8. Something like that, we're going to take and copy all of our code and inside of that Docker container, and to do that we need to create a Docker file. The Docker files can have all the individual steps needed to run our environment. Once that Docker file is created, we can build that Docker file into a Docker image.

That image can be pushed onto a Docker Hub repository, for example, and then we can use that address of our image to create a deployment. The deployment portion of our application is going to involve creating proper manifests and creating the desired state of our application that's going to be running on Kubernetes. Once those applications are running us across Kubernetes, we're going to have to expose the application. What this means is there has to be Internet from the Internet a signal needs to come directly into the pod that contains our container. The deployment we applied in the previous step that we've done by default it's closed off from the Internet, so we're going to have to provide a service and an ingress to that service.

Usually, when we create a service, we're saying these pods are part of this service, and the service will have a domain name registered to an IP address built into Kubernetes. So, something like service a dot Kubernetes dot whatever your domain name is, that is something that every time a signal or a get request is done on that URL, it's going to forward that get request directly to our pod inside of our container. But what happens when we need to do this ingress is that we're accepting HTTP requests and there might need to be different environment variables that need to be turned on or off.

This is where we use something known as a configuration map. What this is basically just a JSON file that contains environment variables so that our image that our application that's running inside of our Docker pod is going to be referencing this configuration map. Usually, these maps are handled by running the file as a volume attached to the container. If you're familiar with Docker, this should be very familiar for you. Another thing you want to do is you want to secure the data using these Secrets. So, before we've mentioned environment variables, you can use things like Redis to inject passwords into our application.

We could use environment variables in the deployment, but these deployment files we shouldn't have any Secrets that are baked into our deployment YAML files because they're going to be on our GitHub page for example or some type of repository. It's a high-security risk to include those, say passwords or IP addresses or Secrets that shouldn't be anywhere on the Internet. They should be stored securely on your machine. One way to handle this is to have those Secrets in a separate YAML file that is loaded directly onto the Kubernetes cluster. And of course, you can deploy this directly into a backend storage using something like a stateful set.

The reason is that we need something like Redis to maintain its state through restart. Just in case those Secrets need to be separated from any running application, they need to be stored as a set on the Kubernetes cluster because as we know, each container or pod or ephemeral that could go down at any time. So, bringing that stuff up and using something like a storage that's deployed directly on the cluster but not on any of the particular pods, we'll make sure that we're not going to be losing any information. So, the other thing you want to do is you want to Add HTML content to the application.

So, far this is just a basic working application, but the only thing that's functioning is the API or something that's like maybe returning a particular message, like hello world for example. Maybe you don't want to have it returned to hello world, maybe you just want to have it return an HTML page. You can do that by adding HTML as part of a static file to your application and then just having the get request route to that HTML file. So, every time you visit the service domain name address offered by Kubernetes, you're going to be getting the HTML file that's offered by your Docker container.

And then you can package our Kubernetes cluster using Helm. Whenever we need to replicate our setup to another environment, maybe we'd like to reapply all the configuration files such as the deployment services, configuration maps, et cetera. We can use the Kubernetes cluster Helm to reproduce everything we've done on this deployment. I can pick up and move to another cloud provider if I so choose or maybe I'd like to do the same thing on-premises solution, this will allow me to do that. That's all the different steps of deploying applications in Kubernetes.

## 8\. Video: Kubernetes Cluster Security (it\_dowwctdj\_03\_enus\_08)

![](https://cdn2.percipio.com/public/b/e1c53319-9ae9-4ee2-ab7d-31bc6e241331/image001.jpg)

In this video, find out how to describe security steps needed when using Kubernetes.

- describe security steps needed when using Kubernetes

\[Video description begins\] *Topic title:Kubernetes Cluster Security. Your host for this session is Elias Zoghaib.* \[Video description ends\]

In this video, we'll be discussing securing Kubernetes clusters and different security features offered by Kubernetes. To begin, you need to first control the Kubernetes API access. This API is what's used to connect or get nodes or create services. Different Kubernetes operations that have to happen on the worker node or the master node requires this Kubernetes API access. Simply put, you want to control who's able to do certain actions using your Kubernetes cluster, and a particular action that you may want to restrict is certain users shouldn't be allowed to retrieve these services, as in do a get services operation or a get nodes operation.

Another thing you want to control is the kubelet access. The kubelet is what's installed on each worker node. It's the piece of software that will communicate with the master system and retrieve any information from the Kubernetes API or the scheduler directly to the kubelet. No outside user should be able to directly connect to the kubelet access. Kubernetes uses TLS or Transport Layer Security for all API traffic and uses basic API authentication, particularly if you're going to be using a small cluster for testing a application with just you as a user, you want to use just a basic TLS API authentication using a bearer auth approach.

However, if you have a larger organization using an LDAP server that allows your users to be subdivided into groups with associated roles for each group will make things a little easier and more secure for you to manage. You want to also control the workload and user permissions. Controlling the capabilities of a workload or user at runtime is very important when it comes to security with Kubernetes. Authorization in Kubernetes is intentionally high-level and focused on course actions on resources. More powerful control exists as policies to limit the use case of how these objects act on the cluster themselves and other resources. Limiting the resource usage on a particular cluster will protect the different cluster components.

Preventing containers from loading unwanted kernel modules is one of the key features of the security network on Kubernetes. Under the thing you want to keep in mind, utilizing role-based access control will help you identify who's allowed to access certain things based on the role that they have associated with their user account. RBAC can help you define who has access to the Kubernetes API and what permissions they have. RBAC is usually enabled by default on Kubernetes 1.6 and higher, but because certain older versions of Kubernetes may require you to enable it. So, I encourage you to check your Kubernetes version before proceeding.

The next thing you want to keep in mind is to secure your etcd. This etcd store, remember, is the thing that holds all information of your Kubernetes operations. Anytime you are asked to create a new service or update a node, or do something along the lines of managing your worker nodes or changing your master system, etcd will record that information. It's crucial then to turn on encryption at rest for etcd secrets. Since encryption is a crucial for securing etcd and it's not turned on by default, you can enable it via what's called a Kube API server process by passing a particular command to the Kube CTL command line.

Next thing you want to keep in mind is you should keep your nodes isolated. Kubernetes nodes must be on separate networks and should not be exposed directly to public networks. Remember, the only thing that the worker node should be able to communicate with is the master node. You communicate with the master node and the master node will communicate with the worker nodes. If possible, you should even avoid direct connections to the general corporate network. It needs to be completely isolated from anything that's available for user to access.

Just the master node system should be able to access these nodes. This is what's meant by isolation. And next thing you want to keep in mind is using third-party authentication for an API server. We've seen this in multiple websites or services, but one of the tools that's great is using GitHub or Google to authenticate a user. Using a third-party or OAuth 2.0 application will allow you to make sure a larger service is able to identify, authenticate, and authorize a particular user for doing a particular action on an API server. You want to also monitor the traffic to limit communications. Monitoring network traffic is crucial.

Containerizing applications generally make extensive use of cluster networks. Observe the active network traffic and compare it to the traffic allowed by Kubernetes network policy. To understand how your applications interacting and identify any anomalous communications, you want to keep your Kubernetes current. Every time there's an exploit available on current versions of Kubernetes, it's updated. You need to make sure that you are using the latest version of Kubernetes so that you have the latest security features, so those known exploits won't work on the current version of Kubernetes.

Finally, you want to understand that you have to enforce your process authorization. This is known as whitelisting. Process whitelisting is an effective way to identify unexpected running processes. First, observe the application over a period of time to identify all the process running during normal application behavior. Then use this list as your white list for future application behavior. If you notice later down the line that there's an extra process that's not normally supposed to be there on your cluster, you'll know that something is wrong.

The next thing you want to do is make sure that your kubelet is locked down and secure. The kubelet is an agent running on each node. This interacts with the containers on runtime to launch Pods and report metrics for nodes and Pods. Each kubelet in the cluster exposes an API which you can use to start and stop Pods and perform other operations. If an unauthorized user gains access to this API on any of the worker nodes, they can run code on the cluster and they can compromise the entire cluster. Make sure that your kubelet is secure. Disable anonymous access, for example, or setting authorization mode to a value other than always allow will make sure your kubelet is secured. Next thing I want to bring up is audit logging.

Make sure that the audit logging is enabled and you are monitoring unusual or unwanted API calls, especially authentication failures. It's known that if a user is authenticating multiple times and they're failing, you'll know that someone is trying to break into something, almost like someone's trying to pick a lock to your house. Identifying those logs and seeing how many authentication failures are happening will tell you if someone's actually trying to break in. The next thing you want to keep in mind is you have to secure the Kubernetes using third-party tools.

Kubernetes clusters with Aqua, for example, is a third-party tool that's used to tame the complexity of Kubernetes security with what's known as Kubernetes Security Posture Management, also known as KSPM, an advanced agentless Kubernetes runtime protection. Aqua is a Kubernetes native to achieve policy-driven full lifecycle protection and compliance for all your Kubernetes application. Kubernetes Security Posture Management will scan your YAML files and scan what's happening to your cluster and let you know automatically when there are breaches to security. So, that's all I have about securing Kubernetes clusters.

## 9\. Video: Kubernetes Use Cases (it\_dowwctdj\_03\_enus\_09)

![](https://cdn2.percipio.com/public/b/c71c6a31-5930-46a6-ae18-cd6999d798d8/image001.jpg)

Upon completion of this video, you will be able to identify when Kubernetes is an appropriate tool.

- identify use cases when Kubernetes is an appropriate tool

\[Video description begins\] *Topic title:Kubernetes Use Cases. Your host for this session is Elias Zoghaib.* \[Video description ends\]

In this video, we'll be discussing Kubernetes use cases. So, we've probably seen Kubernetes before and we're trying to understand why would we even use a tool like this instead of, say, something like Docker. Since it's so portable, you'll be able to just to take a container, put it somewhere, and have somebody access it. Well, keeping containerized apps up and running can be complex because they often involve many containers deployed across many different machines. Using Kubernetes provides a way to schedule and deploy these containers, plus scale them to your desired state and manage their lifecycles.

When we talk about scale, we're referring to scaling with traffic. As a lot of people are trying to access their services, Kubernetes is going to need to spin up more of these Pods, so our code can actually access to everyone who's looking to access our services. Think of this as you have many different cashiers open at the supermarket line. By doing this, a lot more people can get serviced rather than standing in one line and waiting serially for the next person in the line to clear out. Since this will scale containers easily, it will also make your workloads more portable. Because container apps are separate from their infrastructure, they become portable when you run them on Kubernetes.

In fact, you can move them from local machines to production, to on-premises, to even hybrid or multiple cloud environments, all while maintaining consistency across environments. You're guaranteed that the machine will be able to run your code exactly the same way no matter what operating system is running on it. And because of this, you can build more extensible apps. A large open-source community of developers and companies actively builds extensions and plugins that adds capabilities such as security, monitoring, and management. Plus, the certified Kubernetes conformance program requires every Kubernetes version to support these APIs that makes it easier to use these common offerings by the community.

Next thing you want to keep is the lift and shift. What I mean by this is there's a scenario that occurs frequently today as software is migrated from on-prem infrastructure to cloud solutions. Let's imagine basic situation. You have an application, it's deployed on a physical server and just a regular data center. Practical or economic reasons you decided to move this into the cloud, either to a virtual machine or to big Pods in Kubernetes. But moving to big Pods in Kubernetes isn't a cloud-native approach, but it can be treated as an intermediary phase. First, such a big app working outside the cloud is moved to the same big app in Kubernetes. After that, you've got your Kubernetes ready to go.

You can then lift and shift the Kubernetes application that's running in your cluster to a cloud-managed cluster, and this provides what's known as a microservices architecture. This use case is where you want to deploy more complicated app with many different components that will communicate with one another is a classic scenario for using Kubernetes. In fact, its origins go back to Google deploying, managing, and scaling apps in a more efficient way by using these containers. This is how the whole container orchestration platform was born. Kubernetes is just one of the tools that Google developed to manage these microservices. You can think of a microservice as a single-responsibility piece of code that runs inside of a container.

Maybe it's an ordering for a particular online store. When you order, you're creating an order that is considered the order microservice. All it does is take your money and send the order information to say, someone like shipping. A shipping microservice will receive that information from your order microservice and begin the process of shipping your goods to you. It's just a basic example of what microservices are, and if your code and your thinking of your architecture is a lot like that, Kubernetes is definitely for you. Another thing you want to focus on like Kubernetes is great with is IoT, the Internet of Things deployment. This means is you have say a thermometer that's connected to the Internet, an IoT application Platform as a Service.

This simplifies and accelerates this IoT solution development and operations. Since IoT requires this platform, it can connect too many different devices that are connected through the Internet and read their state is what it's referred to. So, a thermometer reading a particular temperature reading could be considered a state and it can ping that information over and over again and retrieve that info in a Kubernetes cluster. So, all that's happening is that Kubernetes has a bunch of addresses to all these devices connected to the Internet, say, it's a bunch of thermometers and a restaurant, for example. It will constantly ping these thermometers and send information back to the Kubernetes server for it to be processed.

Another thing you want to keep in mind is Kubernetes is self-healing. Imagine you had a Kubernetes process that can be detailed as Pods and services and because a Pod is ephemeral, it can go down and it could not be in the desired state. Maybe I need three Pods running at all times and now I only have one or two. Since, Kubernetes is self-healing, it'll just spin up those extra two Pods, it'll restart everything, and continue as normal and I won't even notice the downtime. The next thing is that it's a serverless architecture.

Serverless architecture has taken the world by storm and this principle is simple, it's just developed the code, don't worry about everything else, Kubernetes will handle all of the provisioning, all of the tools. Everything it needs to handle what machine will be running it, how will it communicate with the other machines is all handled by the Kubernetes software. Next thing I want to bring up is that you can use machine learning. Imagine that you're building a machine learning model, and because these techniques are very difficult, they're computationally intensive. You can actually have containers running the training simulation. So, the process of building an effective AI model and using it in production is complicated and time-consuming. But let's say that you had a built-in model.

You can build that application to use that model, and place the container inside of a Pod, and make the Pod part of services where people can use your machine learning model at the inference step. Meaning if I bring in data, what will its prediction be? Kubernetes will handle all that on its own. Next thing you want to talk about is the cloud-native network. This is fundamental. Few years ago, big companies had major issues. The network services were based on hardware such as firewalls or load balancers and they provided specialized hardware companies. Of course, this left them dependent on these hardware providers, and because we're using what's more of an open-source framework offered by a cloud provider, it's almost universal.

Doesn't matter what hardware you're using, if your code can run on this bit of hardware that's on-premises, it will also run in the cloud natively. And of course, there is the CI/CD Software Development Life Cycle. Imagine if you're building things and you're constantly merging your code into a particular branch on GitHub or Bitbucket. You can develop CI/CD tools to build your container, push that image to image registry on a cloud platform, for example, and deploy using that image that's on that image registry directly to your Kubernetes cluster, all in one step.

These pipeline tools are available to interface directly with Kubernetes and it's what I use on my day-to-day work. And of course, there's computing power for resource-hungry tasks. This HPF is what's really important. High performance computing is very difficult and because Kubernetes can scale, you can actually add more worker nodes. And no matter how much resources are going to be consumed by your application or based on the traffic, you can just have Kubernetes add more worker nodes and it will then use those resources on each worker node and split them across so that you're constantly hitting that desired state without any lag time.

## 10\. Video: Kubernetes vs. Docker (it\_dowwctdj\_03\_enus\_10)

![](https://cdn2.percipio.com/public/b/006b9029-dba3-415a-be9d-3d335425965d/image001.jpg)

During this video, you will learn how to compare and contrast Kubernetes with Docker.

- compare and contrast Kubernetes with Docker

\[Video description begins\] *Topic title:Kubernetes vs. Docker. Your host for this session is Elias Zoghaib.* \[Video description ends\]

In this video, we'll be discussing the difference between Docker and Kubernetes. First, let's begin with a basic example of what Docker is. Docker is an open-source technology that uses a open-source container format to automate the deployment of applications as portable, self-sufficient containers that can run in the cloud or on-premises on any operating system. Docker shares a similar name as one of the companies that cultivates the open-source Docker technology to run on Linux and Windows.

You're going to see that Docker is the lifeblood of Kubernetes, but it's there are slight differences of what's actually happening. Since, it's great for automated application deployment. We're going to be using different tools to connect Docker with Kubernetes. We'll explain what that is in just a few moments. But first, let's explain some key Docker features that are there. Docker comes with developer tools, and they're used to build, share, and run orchestrate containerized apps. Developer tools for building container images.

Docker builds a container image, which is known as the blueprint for a container and includes everything needed to run an application, application encode the binaries, the scripts, the dependencies, even the operating system. This configuration and environment variables are also included, and Docker compose is a tool for using this information to create containers out of the images. So, if an image is just a blueprint and Docker is just going to use that blueprint to create a bunch of containers. What happens when it runs?

Since containers are running, Docker Engine will take that image, create a bunch of ephemeral containers based on your application that has all of the needed information needed to run your application, and will deploy that on the Docker Hub. This Docker container that's actually installed on your machine is actually running on its own kernel and you can actually scale these similar to what Kubernetes does, what's known as Docker Swarm. This manages a cluster of these Docker Engines, typically on different nodes called the swarm, and this is where the similarity between Docker and Kubernetes comes in.

Another thing that's great is you can share images using Docker Hub. Docker Hub is just a registry service that's provided by Docker for finding and sharing container images with your team or even the public. It's almost like the AWS ECR, the Elastic Container Registry, or the equivalent of other cloud providers will offer their own container registry for you to use just within your organization or deploy them anywhere to anyone. This image sharing is offered by their own website called Docker Hub, rather than through a cloud provider. It has open-source orchestration software. Since, Kubernetes is just an open-source orchestration software that provides an API to control how and where those containers will run.

It allows you to run your Docker containers and workloads and helps you to tackle some of the operating complexities when moving them to scale multiple containers. There's also the API for container management. This Kubernetes lets you use this API to manage these different containers. So, I can use kubectl which will make these API calls to my system, my cluster, and my master node is going to read those API calls and then manage the containers. Maybe I want to turn off this container, or turn on an extra container, or make another pod with a few more containers inside of it.

I can do all of that using its API. Here's some key Kubernetes features, there's the automation. Since it's such a great tool for automating your deployments. You can use your Docker image and create many different containers and many different pods and Kubernetes will manage those pods running your containers. You tell it a desired state and it will always match that desired state. And it's very flexible. I can take away worker nodes, if I have too many or I can add worker nodes and it will scale just accordingly. And speaking of scalability, if I notice that my traffic is starting to spike.

And I need more worker nodes or more containers, I can let Kubernetes just handle the scalability on its own. When traffic starts to come in, I'll say I need to have three to five containers, 5 during the maximum traffic periods, and when you notice the traffic dipped down, scale it back down to three, but always stay between 3 to 5 as my desired state. Kubernetes will automatically handle any process that it needs to, to make sure that it's in the desired state. There are some advantages for using Docker Swarm over Kubernetes, so since both are production-grade container orchestration platforms, some of them will have different strengths and weaknesses. Docker Swarm for example is also referred to as Docker, in swarm mode is the easiest orchestrator to deploy and manage.

It's probably the greatest one to use if it's your first time using any type of orchestration tool. It can be a good choice for an organization just getting started if you're just a startup and you're not really sure if you want to spend the extra money on cloud or something advanced like Kubernetes, starting off with Docker Swarm just to test the waters will be a good choice for you. Swarm solidly covers 80 percent of all use cases with 20 percent of Kubernetes complexity, so it's less complex for that. And it integrates with Docker tools already, since it seamlessly integrates with the rest of the Docker tool suite, such as Docker compose and the Docker command line interface. It provides a familiar user interface through Docker images or Docker compose, and even through Docker Swarm.

You're going to be using the same tool set, you don't actually have to switch between them, and that's great. If you only know one, this will be the great one to jump on for. There are some Kubernetes advantages though. The fact that it has much broader support than Docker Swarm is one of them. This is awesome when you're trying to think of something like the Amazon Elastic Kubernetes service, or what's called EKS, or the Azure Kubernetes service, or the Google Kubernetes platform. They each offer their own managed Kubernetes service. I can just go to the cloud and provision a cluster and from there it'll just scale and configure accordingly based on my settings that I've told it to. I don't actually have to worry about the minutia of the details of actually setting up the clusters myself. I just tell the cloud exactly what I want with how much RAM on each node, how many CPUs on each node, and the master node and the worker node will work together to make sure that I'm meeting the desired state of my application.

Let's talk a bit about some Docker Swarm and Kubernetes differences. How does advance observability benefit Docker Swarm? Well, let's start by looking at the installation process. Installing Kubernetes is very simple, you can just install the information accordingly without having any issues. Using just a simple pseudo command on Ubuntu or going to the Docker website and installing Docker will give you the Docker Engine and you're able to create Docker images. And a bunch of containers to run on the Docker Engine. The complexity of each of these tools is well, it's a little different. Since Kubernetes is far more complex, but Docker Swarm is less complex, but it covers 80% of the use cases.

You might be better off just using Docker Swarm for a small application that you're starting, but if you're trying to scale an application out to millions of people, you're going to need the cloud for that, using the extra complexity on Kubernetes on the cloud platforms will be the way to go. And there's different scaling techniques. For example, Kubernetes will scale, but it's a little harder for Docker Swarm to scale. You have to, it's not as automatic to say you have to make sure that you're setting a desired state directly on Kubernetes, whereas with Docker Swarm it doesn't necessarily have that kind of capability.

And this your monitoring both offers basic monitoring, but the cloud support will offer extensive monitoring using log systems. And then there's the load balancing. Remember with the whole idea of Kubernetes is to use a load balancer to understand the traffic that's coming in and split them across the different pods that are running your applications. You don't want to have one pod taking on 90% of the workload or is the other two are barely taking 10% in total, having a load balancer split them evenly to about 33% each for three pods for example, or 25% each for four pods, for example, will be the best way to go.

Docker Swarm doesn't really offer a built-in load balancer, at least not out-of-the-box. You'll have to use something like NGINX yourself. Kubernetes Master system does come with one, and it's very useful for when you're trying to deploy for millions of people. And then there's the CLI. Both offer a CLI tool, and they're very expensive. Since, the CLI is probably the most familiar to anyone who's used Docker, Kubernetes will be very simple to transition from a Docker CLI to a Kubernetes CLI. The same kind of principles apply to both. So, those are just some of the differences between Docker Swarm and Kubernetes.

## 11\. Video: Course Summary (it\_dowwctdj\_03\_enus\_11)

![](https://cdn2.percipio.com/public/b/71ee3a54-eeab-41f2-9478-55062c92c9d2/image001.jpg)

In this video, we will summarize the key concepts covered in this course.

- summarize the key concepts covered in this course

\[Video description begins\] *Topic title: Course Summary* \[Video description ends\]

So, in this course, we've examined Kubernetes containers and Kubernetes workflow and situational usefulness. We did this by exploring Kubernetes services, terms, and navigating the workflow. We've used the Kubernetes desired state, provisioning, and deploying applications.

And we've seen Kubernetes use cases and comparisons with Docker. In our next course, we'll move on to the basics of Chef for DevOps.